{
 "metadata": {
  "name": "",
  "signature": "sha256:dc6ffa3fc293abf7f7ce19c3c724736b6c41c994550d297de7db2856f841f138"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Constructing a Hausa Corpus From BBC News Articles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*TLDR*; doing NLP with non-mainstream languages is hard because libraries, corpora and taxonomies, created by people speaking a small subset of global languages, don't generally include these languages. Often the first step in NLP is actually recognising the language being spoken. This notebook steps through how I derived a corpus of Hausa language to train a language classification algorithm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a great time to be doing Natural Language Processing (NLP). Anyone can now access previously unthinkable quantities of text data to efficiently mine and train algorithms on, not only social media such as Twitter but also news, Wikipedia and others. Spare a thought for the students of [Zipf](), pioneer of statistical language analysis, who were used as 'human machines' to manually count the frequency of occurence of each letter in large corpora (see this great historical [overview](http://bds.sagepub.com/content/1/1/2053951714535365.full)). \n",
      "\n",
      "However, this invariably introduces a strong bias towards a certain set of languages spoken in rich countries (see [this great blog post](http://idibon.com/marketing-budgets-and-language-diversity/) from the global language technology firm [Idibon](http://idibon.com)). This is problematic when this kind of analysis is used to draw very broad sociological conclusions (see these great analyses of the potential fallacies of this [Big Questions for Social Media Big Data](http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/download/8062/8151) and [Big Data, Machine Learning and the Social Sciences](https://medium.com/@hannawallach/big-data-machine-learning-and-the-social-sciences-927a8e20460d)). However, when conducting analysis of social media content for development work, these kind of shortcomings become more than an unfortunate bias and actually stop you doing your work."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a small step to begin to correct this problem this notebook steps through how one might construct a corpus in a non-mainstream language (specifically [Hausa](http://en.wikipedia.org/wiki/Hausa_language), a language of West Africa). The raw content is derived from the ever invaluable multi-lingual content provided by the [BBC](http://www.bbc.co.uk/hausa). The URLs of a number of news stories are taken from the [BBC Hausa Twitter timeline](https://twitter.com/bbchausa) using the Twitter API. The raw content of the pages are scraped using Python's [Beautiful Soup](https://pypi.python.org/pypi/BeautifulSoup). A follow up notebook will cover how to train the excellent open source language detection module [langid](https://github.com/saffsd/langid.py) on this corpus so that new Hausa text can be automatically detected.\n",
      "\n",
      "*A Final Word*: Scrape responsibly! While the BBC has a good infrastructure that can withstand a lot of requests, not all sites can. So think before you set off a large scraping script, and at least throttle your requests by sleeping inbetween hits. Not only is this good karma, you are less likely to have your IP blacklisted by a sys admin somewhere."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Dependencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only non-core Python module used here is [Beautiful Soup](https://pypi.python.org/pypi/BeautifulSoup). The great notebooks that accompany [Mining the Social Web](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition) were also invaluable and I used a library of helper functions ```hit_tw_api_harvest.py``` based on these. This requires a Twitter API key (a primer on OAuth and Twitter API access is found [here](https://rawgit.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/master/ipynb/html/_Appendix%20B%20-%20OAuth%20Primer.html))"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re,sys,os,re,csv,time\n",
      "import urlparse\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib,urllib2\n",
      "import hit_tw_api_harvest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Load up the Twitter API and grab tweets by BBC Hausa account"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw=hit_tw_api_harvest.oauth_login()\n",
      "res=hit_tw_api_harvest.harvest_user_timeline(tw,screen_name='bbchausa',max_results=999999999)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Fetched 200 tweets\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Done fetching tweets\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Grab the URLs from each tweet (if they are there)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rootUrls=[]\n",
      "for r in res:\n",
      "    for l in r['entities']['urls']:\n",
      "#        print l['url']\n",
      "        rootUrls.append(l['url'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function that does basic cleaning; removes punctuation and makes lower case. This leaves raw content ready to be parsed for 3 character sequences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(t):\n",
      "    t=t.lower()\n",
      "    t=re.sub(';|:|\\.|\\\"|\\?|\\(|\\)|\\,',' ',t,re.U)\n",
      "    return t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert clean(\"?Test.\")==u' test '"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Grab the text from the p elements of the story body**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a quick and dirty strategy, we look for the body of the story in the standard article template. If it isn't there then we simply ignore the article"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getText(t,outFile,sleep=True,verbose=False):\n",
      "    '''Pass in raw text and file to write to\n",
      "    Optional Boolean flags for sleeping between hits \n",
      "    and verbose logging\n",
      "    '''\n",
      "    tt=BeautifulSoup(t.read())\n",
      "    \n",
      "    element=tt.find(class_='story-body__inner')\n",
      "    \n",
      "    if verbose:print 'GETTING',t.url\n",
      "    if sleep:time.sleep(1)\n",
      "    \n",
      "    if element:\n",
      "        for p,pp in enumerate([d for d in element.find_all('p') if d]):\n",
      "            outFile.write(clean(pp.get_text()).encode('utf-8')+' ')\n",
      "        outFile.write('\\n') # One line per story\n",
      "    else:\n",
      "#        print '\\tNO DOCUMENT BODY'\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('out_bbc_hausa.txt','w') as outFile:\n",
      "\n",
      "    for n,url in enumerate(rootUrls):\n",
      "        if n%100==0:print n,url\n",
      "        t=urllib2.urlopen(url)\n",
      "        getText(t,outFile,sleep=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 http://t.co/rRaU3BAdDd\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/99nOzrqDRJ\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/bGiDhZAOB0\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/nriNnpQrCB\n",
        "400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/QUWo1SMB97\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/xLhXll1Uej\n",
        "600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/9V5ToStTJ3\n",
        "700"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/N3XS2aBQDf\n",
        "800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/8X1TJpCSCD\n",
        "900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/LsMEuDCrzW\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/Rab4dB44nV\n",
        "1100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/IkJG4Me05s\n",
        "1200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/bRDaiSnRXF\n",
        "1300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/x98a6sI4Zz\n",
        "1400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/sCkJIUnViF\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/KYXDlxEb9B\n",
        "1600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/FaNJKuHJJv\n",
        "1700"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/SurNkGoOSl\n",
        "1800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/59UYVpR22w\n",
        "1900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/uNzeFIB8Ur\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/dgbP1x7Yu5\n",
        "2100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/K2omMOh358\n",
        "2200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/YoTK5j5bmd\n",
        "2300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/SnMWNoqpte\n",
        "2400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/qHh9Umsq0o\n",
        "2500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/Ro6SqiB5qv\n",
        "2600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/v9Fe2kkMrk\n",
        "2700"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/BexRDOULGf\n",
        "2800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/fr07rCF4SF\n",
        "2900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/2opK4htjn6\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://t.co/vSviW81OnM\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have one story per line: 2779 stories in total and 411,919 words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc out_bbc_hausa.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    2779  411919 2268635 out_bbc_hausa.txt\r\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take a look at the first couple of stories"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n 2 out_bbc_hausa.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kamfanin jiragen sama na airasia  ya ce tarkacen da aka gano a cikin tekun na jirgin fasinjarsa ne da ya yi batan dabo a ranar lahadi  jirgin na airasia dauke da mutane 162 ya bace ne a kan hanyarsa daga surabaya na indonesia zuwa singapore  shugaban kasar indonesia  joko widodo ya je wurin da jirgin ya yi hadarin inda aka gano gawawwaki da dama  mr widodo ya ce zai aike da karin jiragen ruwan soji wurin  inda igiyar tekun ke kawo cikas wajen ci gaba da laluben  a na shi bangaren shugaban kamfanin airasia tony fernandes  ya ce ya kadu ainun da hadarin jirgin kuma ya yi alkawarin taimakawa 'yanuwan wadanda hadarin ya rutsa da su  \r\n",
        "newcastle united ta sanar da sunan alan carver da steve stone a matsayin wadanda za su jagoranci kulob din a wasanni biyu na gasar premier  kulob din ya dauki wannan matakin ne domin maye gurbin alan pardew wanda yake tattaunawa da crystal palace a matsayin sabon kociyanta  tuni kulob din ya sanar da cewar pardew ba shi ne zai jagoranci atisayen \u2018yan was a ranar talata ba  inda carver da stone ne za su fara aikinsu  za kuma su jagoranci newcastle buga gasar premier da za su fafata da burnley ranar alhamis da kuma kofin kalubale da za su buga da leicester ranar asabar  \r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Next Steps and New Directions\n",
      "1. **Train [langid](https://github.com/saffsd/langid.py) on this content** - Train a language classifier on the distribution of 3 character sequences and test performance (forthcoming in another notebook)\n",
      "2. **More content** - Grab more stories from the BBC Hausa page, maybe using a crawler.\n",
      "3. **More sources** - Grab content from other Hausa sources\n",
      "4. **Baseline Comparison** - How does the clean language from official news compare to how people use the language in everday communications?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you liked this, then please [let me know](https://twitter.com/arutherfordium)! If you also work on under-represented languages then let me know what you are doing; at [United Nations Global Pulse](unglobalpulse.org) we often work on problems like this and we frequently need help from native speakers. Thanks for reading!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}