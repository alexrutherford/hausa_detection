{
 "metadata": {
  "name": "",
  "signature": "sha256:2f2a4a969a8e0036a0feb00654867daa8fe54ed50515df761b9d6d7bae745448"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Constructing a Hausa Corpus From BBC News Articles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*TLDR*; doing NLP with non-mainstream languages is hard because libraries, corpora and taxonomies are created by people speaking a small subset of global languages so don't generally include these languages. Often the first step in NLP is actually recognising the language being spoken. This notebook steps through how I derived a corpus of Hausa language to train a language classification algorithm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a great time to be doing Natural Language Processing (NLP). Anyone can now access previously unthinkable quantities of text data to efficiently mine and train algorithms on, not only social media such as Twitter but also news, Wikipedia and others. Spare a thought for the students of [Zipf](), pioneer of statistical language analysis, who were used as 'human machines' to count the frequency of occurence of each letter in large corpora (see this great historical [overview](http://bds.sagepub.com/content/1/1/2053951714535365.full)). \n",
      "\n",
      "However, this invariably introduces a strong bias towards a certain set of languages spoken in rich countries (see [this great blog post](http://idibon.com/marketing-budgets-and-language-diversity/) from the global language technology firm [Idibon](http://idibon.com)). This is problematic when this kind of analysis is used to draw very broad sociological conclusions (see these great analyses of the potential fallacies of this [Big Questions for Social Media Big Data](http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/download/8062/8151) and [Big Data, Machine Learning and the Social Sciences](https://medium.com/@hannawallach/big-data-machine-learning-and-the-social-sciences-927a8e20460d)). However, when conducting analysis of social media content for development work, these kind of shortcomings become more than an unfortunate bias and actually stop you doing your work."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a small step to begin to correct this problem this notebook steps through how one might construct a corpus in a non-mainstream language (specifically [Hausa](http://en.wikipedia.org/wiki/Hausa_language), a language of West Africa). The raw content is derived from the ever invaluable multi-lingual content provided by the [BBC](http://www.bbc.co.uk/hausa). The URLs of a number of news stories are taken from the [BBC Hausa Twitter timeline](https://twitter.com/bbchausa) using the Twitter API. The raw content of the pages are scraped using Python's [Beautiful Soup](https://pypi.python.org/pypi/BeautifulSoup). A follow up notebook will cover how to train the excellent open source language detection module [langid](https://github.com/saffsd/langid.py) on this corpus so that new Hausa text can be automatically detected.\n",
      "\n",
      "*A Final Word*: Scrape responsibly! While the BBC has a good infrastructure that can withstand a lot of requests, not all sites can. So think before you set off a large scraping script, and at least throttle your requests by sleeping inbetween hits. Not only is this good karma, you are less likely to have your IP blacklisted by a sys admin somewhere."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Dependencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only non-core Python module used here is [Beautiful Soup](https://pypi.python.org/pypi/BeautifulSoup). The great notebooks that accompany [Mining the Social Web](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition) were also invaluable and I used a library of helper functions ```hit_tw_api_harvest.py``` based on these. This requires a Twitter API key (a primer on OAuth and Twitter API access is found [here](https://rawgit.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/master/ipynb/html/_Appendix%20B%20-%20OAuth%20Primer.html))"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re,sys,os,re,csv,time\n",
      "import urlparse\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib,urllib2\n",
      "import hit_tw_api_harvest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Load up the Twitter API and grab tweets by BBC Hausa account"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw=hit_tw_api_harvest.oauth_login()\n",
      "res=hit_tw_api_harvest.harvest_user_timeline(tw,screen_name='bbchausa',max_results=999999999)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Fetched 200 tweets\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Done fetching tweets\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Grab the URLs from each tweet (if they are there)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rootUrls=[]\n",
      "for r in res:\n",
      "    for l in r['entities']['urls']:\n",
      "#        print l['url']\n",
      "        rootUrls.append(l['url'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Grab the text from the p elements of the story body**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function that does basic cleaning; removes punctuation and makes lower case. This leaves raw content ready to be parsed for 3 character sequences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(t):\n",
      "    t=t.lower()\n",
      "    t=re.sub(';|:|\\.|\\\"|\\?|\\(|\\)',' ',t,re.U)\n",
      "    return t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert clean(\"?Test.\")==u' test '"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a quick and dirty strategy, we look for the body of the story in the standard article template. If it isn't there then we simply ignore the article"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getText(t,outFile,sleep=True,verbose=False):\n",
      "    '''Pass in raw text and file to write to\n",
      "    Optional Boolean flags for sleeping between hits \n",
      "    and verbose logging\n",
      "    '''\n",
      "    tt=BeautifulSoup(t.read())\n",
      "    \n",
      "    element=tt.find(class_='story-body__inner')\n",
      "    \n",
      "    if verbose:print 'GETTING',t.url\n",
      "    if sleep:time.sleep(1)\n",
      "    \n",
      "    if element:\n",
      "        for p,pp in enumerate([d for d in element.find_all('p') if d]):\n",
      "            outFile.write(clean(pp.get_text()).encode('utf-8')+' ')\n",
      "#            print pp\n",
      "        outFile.write('\\n') # One line per story\n",
      "    else:\n",
      "        print '\\tNO DOCUMENT BODY',t.url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('out_bbc_hausa.txt','w') as outFile:\n",
      "\n",
      "    for n,url in enumerate(rootUrls):\n",
      "        if n%25==0:print n,url\n",
      "        t=urllib2.urlopen(url)\n",
      "        getText(t,outFile,sleep=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 http://t.co/f9SxxAR2uP\n",
        "\tNO DOCUMENT BODY"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.independent.co.uk/sport/football/premier-league/manchester-united-defender-luke-shaw-renting-385m-home-from-cristiano-ronaldo-9946953.html\n",
        "25"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc out_bbc_hausa.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    2801  417619 2301034 out_bbc_hausa.txt\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head out_bbc_hausa.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Rahotanni daga birnin New York na Amurka, na cewa wani mutum da ya rubuta sakon nuna kin jin 'yan sanda a dandalin sada zumunta da musayar ra'ayi, ya harbe wasu 'yan sanda biyu har lahira, sannan daga baya shima ya harbe kansa. Kwamishinan 'yan sandan jihar, Bill Bratton, wanda ya tabbatar da aukuwar lamarin, ya ce mutumin ya harbe 'yan sandan ne yayin da suke zaune a cikin motarsu, kuma saboda suna sanye ne da kayan 'yan sanda. Magajin Garin na New York, Bill de Blasio, ya ce duk wanda ya ga an saka wani sako dake nuna barazana ga 'yan sanda a dandalin sada zumunta, to ya yi maza ya sanar da hukuma. Kisan 'yan sandan na zuwa ne a daidai lokacin da jama'a da dama ke ke ci gaba da nuna fushinsu bisa kin hukunta 'yan sanda fararen fata dake kashe bakaken fatan da basa dauke da makamai. \r\n",
        "A Najeriya, jama'ar Dambua dake jihar Borno a arewa maso gabashin kasar sun ce sojoji da mutanen gari sun kashe kimanin 'yan kungiyar Boko Haram 115 a ranar Juma'a. Wasu mazauna garin suka ce maharan sun isa garin ne lokacin da ake shirin gudanar da sallar Juma'a, inda suka firgitar da jama'a da harbe-harbe. Harin ya zo ne kasa da mako daya da wasu maharan suka kashe mutane 33 da sace wasu 200 a Gumsuri dake cikin karamar hukumar ta Dambua. Karo na uku kenan da 'yan kungiyar Boko Haram ke kokarin kai hari garin na Dambua tun bayan da sojoji suka kwato garin daga hannanun 'yan kungiyar a cikin watan Satumba, bayan wata daya yana hannunsu. A wani labarin kuma, rahotannin sun ce 'yan kungiyar ta Boko Haram sun kashe jami'in dan sanda guda lokacin wani hari ranar Juma'a a kauyen Damagun, dake karamar hukumar Fune a jihar Yobe. \r\n",
        "Sakatare Janar na Majalisar Dinkin Duniya Ban Ki-Moon na cigaba da ziyara a kasashen da ke fama da cutar Ebola. Ya ziyarci Guinea, kuma ana saran zai je Mali a yau kafun yaje Ghana inda Majalisar Dinkin Duniyar keda cibiyar yaki da cutar. A jiya juma'a, Ban Ki-Moon ya jinjina wa jama'ian Majalisar, musaman ma wadanda cutar ta hallaka, a bisa sadaukar da kansu da suka yi wajen yaki da ita. Mr Ban ya kuma yi alkawarin cewa Majalisar zata cigaba da taimakon da ta ke yi wajen yaki da cutar. Yace: \"kamata yayi ace ganin bayan cutar kwata-kwata ya zama burin kowa da kowa\". Sama da mutane dubu bakwai ne citutar ta hallaka a kasashen yammacin Afrika. \r\n",
        "A Najeriya, jama'ar Dambua dake jihar Borno a arewa maso gabashin kasar sun ce sojoji da mutanen gari sun kashe kimanin 'yan kungiyar Boko Haram 115 a ranar Juma'a. Wasu mazauna garin suka ce maharan sun isa garin ne lokacin da ake shirin gudanar da sallar Juma'a, inda suka firgitar da jama'a da harbe-harbe. Harin ya zo ne kasa da mako daya da wasu maharan suka kashe mutane 33 da sace wasu 200 a Gumsuri dake cikin karamar hukumar ta Dambua. Karo na uku kenan da 'yan kungiyar Boko Haram ke kokarin kai hari garin na Dambua tun bayan da sojoji suka kwato garin daga hannanun 'yan kungiyar a cikin watan Satumba, bayan wata daya yana hannunsu. A wani labarin kuma, rahotannin sun ce 'yan kungiyar ta Boko Haram sun kashe jami'in dan sanda guda lokacin wani hari ranar Juma'a a kauyen Damagun, dake karamar hukumar Fune a jihar Yobe. \r\n",
        "Rahotanni daga Gabas ta Tsakiya na cewa wani jirgin yakin Isra'ila ya kai wasu hare hare ta sama a yakin Zirin Gaza. Bayanai sun ce hare-haren Isra'ilan martani ne ga rokar da aka harba daga yankin na Gaza zuwa yankunan Yahudawa. Wannan shine hari na farko da Isra'ila ta kai tun bayan kawo karshe samamen da ta kaddamar a yankin na Zirin Gaza a watan Agusta. Mazauna yankin Gaza sun ce sun ji karar fashewar wasu abubuwa masu karfi har sau biyu. Jami'ai sun ce ba a samu wadanda farmakin ya shafa ba. \r\n",
        "Shugaba Barack Obama ya ce Amurka za ta maida martani ga Korea ta Arewa, saboda satar bayanai da ta yi a kamfanin Sony pictures mai shirya fina-finai. Mr Obama ya kara da cewa ba za su zura ido suna kallo masu mulkin-kama karya suna yi musu shishshigi a harkokinsu ba. Ya kuma ce martanin da Amurkan za ta mayar zai kasance daidai da abinda Korea ta aikata. Har ila yau Obama ya ce kamfanin Sony Pictures ya yi kuskuren soke fitar da fim da yake yin shagube ga shugaban Korea ta arewan Kim jong Un. Sai dai kamfanin ya kare kansa inda ya ce wannan matakin da ya dauka ya zama dole domin manyan gidajen sinima da dama a Amurka sun ce ba za su nuna fim din ba. \r\n",
        "Apple ya ce ya yi matukar rashin jin dadi game da binciken da BBC ta gudanar akan yanayin da ma'aikata ke aiki a masana'antun dake samarwa da kamfanin kayayyaki. Shirin na BBC Panorama ya shaida cewa ana take ka'idar aiki wajen sa'oin da ma'aikata suke dauka suna aiki da kuma wajen bayar da katin shaidar aiki da tarurrukan aiki. A wani sakon email da kamfanin ya aikewa ma'aikatansa, babban jami'in kamfanin Jeff Williams ya ce bai san wani kamfanin da ya ke yin irin abubuwan da Apple yake yi ba, domin inganta yanayin aikin ma'aikata. Amma sai dai ya kara da cewa: '' Muna iya yin fiye da haka.'' Editan shirin Panorama na BBC Ceri Thomas ya ce yana tsaye kai da fata akan aikin binciken da su ka yi. Ya ce sun gano cewa yara kanana suna aiki a mahakar ma'adinin tin masu matukar hatsari a Bangka da kuma Indonesia Yayinda Apple ya tabbatar da cewa da farkon wannan shekarar yana samun ma'adin tin din daga Bangka, sai dai jami'in ya ce ba a taba tabbatar da cewa ko haramtaccen ma'adin tin na shiga cikin abubuwan da ake samar musu ba Apple dai tun farko ya ki amincewa ya yi hira da shirin na BBC. \r\n",
        "Shugaban Amurka Barack Obama ya sha alwashin za su maida martani game da kutsen da ake zargin Koriya ta Arewa da aikatawa. Obama ya kuma kara da cewa kamfanin Sony wanda kutsen ya shafa ya yikuskure da ya soke kaddamar da wani shiri da zai nuna yadda aka kashe shugaban kasar Koriya ta Arewa, wato Kim Jon-un. A ranar juma'a ne hukumomin Amurkan suka danganta koriyar da kutsen wanda ya fitar da bayanai masu muhimmanci baina a jama'a. Hakan ya sa kamfanin na Sony ya janye shirin bayan wannan barazana da ya samu. \" Za mu maida martani\" Obama ya bayyanawa manema labarai a ranar juma'a ba tare da ya bayyana matakin da Amurkan za ta dauka ba. Ya kuma kara da cewa \"ba za mu amincewa wani dake waje ya dinga tace mana finafinanmu ba.\" A cewar Mr. Obama ya na da matukar muhimmanci kundin adana bayanai na hukuma da ma na jama'a su kasance a tsare saboda muhimmancinsu akan tattalin arziki da rayuwar jama'a ta yau da kullum. Ya kuma jaddada cewa kuskure ne da aka dage kaddamar da shirin fim din. \"Amurka ba za ta canza yanayiN rayuwarta ba saboda gudun harin 'yan ta'addanci.\" \r\n",
        "A Najeriya yanzu haka an soma amfani da sabuwar takardar kudi ta Naira 100 a hukumance wadda gwamnatin Nijeriyar ta bullo da ita. Hukumomin kasar dai sun ce za a ci gaba da amfani da tsohuwar takardar Naira 100 da kuma sabuwar da aka samar a lokaci guda. An kirkiro da sabuwar takardar kudin ta Naira 100 domin tunawa da cikar kasar shekaru 100 da kafuwa. Hakazalika shigowar sabuwar takardar kudin na zuwa ne a daidai lokacin da darajar kudin Nigeriar ke ci gaba da faduwa sakamakon faduwar farashin mai a kasuwar duniya. \r\n",
        "Majalisun dokokin jihohin Nigeria sun yi fatali da batun bai wa kananan hukumomi 'yancin cin gashin kansu a yukunrin da ake na yi wa kudin tsarin mulkin kasar kwaskarima. Hakan na kunshe ne a cikin rahoton da majalisun dokokin jihohin kasar suka mikawa majalisar dokokin tarayya ranar jumu'a a Abuja game da gyare- gyaren kundin tsarin mulkin kasar. Su dai 'yan majalisar dokokin kasar na tarayya sun amince a bai wa kananan hukumomin 'yancin cin gashin kansu a mahawarar da suka tafka. Kuma a cikin watan Oktoba ne dai majalisar dokokin ta tarayya ta aikewa majalisun jihohin kundin tsarin mulkin da aka yiwa gyaran fuska, domin su amince da gyare gyaren da ta yi. Batun bai wa kananan hukumomi 'yancin cin gashin kansu ya janyo zazzafarar mahawara a bakin 'yan kasar. \r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}